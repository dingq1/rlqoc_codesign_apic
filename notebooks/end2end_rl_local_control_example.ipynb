{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djXLDIS0zCBo"
      },
      "source": [
        "#### note: change to a GPU runtim type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXXlGXHqx75Q",
        "outputId": "eee1bec9-914f-45e1-d416-384356acc978"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: optax in /usr/local/lib/python3.11/dist-packages (0.2.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from optax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.11/dist-packages (from optax) (0.1.88)\n",
            "Requirement already satisfied: jax>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from optax) (0.4.33)\n",
            "Requirement already satisfied: jaxlib>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from optax) (0.4.33)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from optax) (1.26.4)\n",
            "Requirement already satisfied: etils[epy] in /usr/local/lib/python3.11/dist-packages (from optax) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.10.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.87->optax) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.27->optax) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.27->optax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.27->optax) (1.13.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (3.2.1)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stable_baselines3-2.5.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable-baselines3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 stable-baselines3-2.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optax pandas gymnasium torch stable-baselines3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tWQ0_DZyBY3"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "import jax.random as jrandom\n",
        "import jax.scipy.linalg as jla\n",
        "import jax\n",
        "import matplotlib.pyplot as plt\n",
        "from jax import vmap\n",
        "from jax import random\n",
        "import numpy as np\n",
        "import optax\n",
        "import time\n",
        "import pandas as pd\n",
        "from jax import grad\n",
        "import math\n",
        "import logging\n",
        "from collections import deque, defaultdict\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from typing import Optional\n",
        "\n",
        "import gymnasium as gym  # Updated import for Gymnasium\n",
        "from gymnasium import spaces\n",
        "import torch\n",
        "import torch.nn as nn  # Ensure nn is imported correctly\n",
        "import optax\n",
        "from stable_baselines3.common.policies import ActorCriticPolicy\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "\n",
        "import pickle  # For saving model parameters\n",
        "\n",
        "# ========================================\n",
        "# Logging Configuration\n",
        "# ========================================\n",
        "\n",
        "# ----- Logging Configuration (Print to Console) -----\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # Suppress TensorFlow/XLA logs\n",
        "\n",
        "# Create a logger and set level to INFO\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Remove any existing handlers\n",
        "if logger.hasHandlers():\n",
        "    logger.handlers.clear()\n",
        "\n",
        "# Create a stream handler that outputs to sys.stdout\n",
        "stream_handler = logging.StreamHandler(sys.stdout)\n",
        "stream_handler.setLevel(logging.INFO)\n",
        "\n",
        "# Create formatter and add it to the stream handler\n",
        "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "stream_handler.setFormatter(formatter)\n",
        "logger.addHandler(stream_handler)\n",
        "\n",
        "# ========================================\n",
        "# Part 1: Control System Model (PIC + SLM)\n",
        "# ========================================\n",
        "\n",
        "# Constants and Parameters\n",
        "def calculate_Omega_rabi_prefactor(I_mW_per_cm2, Detuning_MHz):\n",
        "    \"\"\"\n",
        "    Calculate the effective Rabi frequency prefactor (MHz) based on laser intensity and detuning.\n",
        "\n",
        "    Parameters:\n",
        "    - I_mW_per_cm2: Laser intensity in mW/cm²\n",
        "    - Detuning_MHz: Detuning in MHz\n",
        "\n",
        "    Returns:\n",
        "    - Omega_effRabi_prefactor_MHz: Effective Rabi frequency prefactor in MHz\n",
        "    \"\"\"\n",
        "    hbar = 1.0545718e-34  # Reduced Planck constant (J·s)\n",
        "    c = 3e8  # Speed of light in vacuum (m/s)\n",
        "    epsilon_0 = 8.854187817e-12  # Permittivity of free space (F/m)\n",
        "    mu0e = 3.336e-29  # |0> <-> |e> (C·m)\n",
        "    mu1e = 3.400e-29  # |1> <-> |e> (C·m)\n",
        "\n",
        "    I = I_mW_per_cm2 * 10  # Convert intensity from mW/cm² to W/m²\n",
        "    E_0 = jnp.sqrt(2 * I / (c * epsilon_0))  # Electric field in V/m\n",
        "    Omega_0e_prefactor = (mu0e * E_0) / hbar\n",
        "    Omega_1e_prefactor = (mu1e * E_0) / hbar\n",
        "    Omega_effRabi_prefactor = (Omega_0e_prefactor * Omega_1e_prefactor) / (2 * Detuning_MHz * 1e6)\n",
        "    Omega_effRabi_prefactor_MHz = Omega_effRabi_prefactor / 1e6  # Convert to MHz\n",
        "    return Omega_effRabi_prefactor_MHz\n",
        "\n",
        "\n",
        "# Grid and atom positions\n",
        "def generate_grid(grid_size=600, grid_range=(-6, 6)):\n",
        "    \"\"\"\n",
        "    Generates a 2D grid using meshgrid for atom positioning.\n",
        "\n",
        "    Args:\n",
        "    grid_size (int): Number of points on the grid (default is 600).\n",
        "    grid_range (tuple): Range of x and y coordinates (default is (-6, 6)).\n",
        "\n",
        "    Returns:\n",
        "    X, Y: Meshgrid for the x and y coordinates.\n",
        "    \"\"\"\n",
        "    x = jnp.linspace(grid_range[0], grid_range[1], grid_size - 1)\n",
        "    y = jnp.linspace(grid_range[0], grid_range[1], grid_size - 1)\n",
        "    X, Y = jnp.meshgrid(x, y)\n",
        "    return X, Y\n",
        "\n",
        "# Function to generate atom positions\n",
        "def generate_atom_positions_equilateral(N_a, side_length=3.0, center=(0, 0), triangle_spacing_factor=2.0):\n",
        "    \"\"\"\n",
        "    Generate atom positions for any integer number of atoms (N_a) in equilateral triangular patterns.\n",
        "\n",
        "    Parameters:\n",
        "    - N_a (int): Total number of atoms.\n",
        "    - side_length (float): Length of each side of the equilateral triangle.\n",
        "    - center (tuple): Center of the first triangle (x_center, y_center).\n",
        "    - triangle_spacing_factor (float): Distance between triangle centers, as a multiple of side_length.\n",
        "\n",
        "    Returns:\n",
        "    - positions (list of tuples): List of (x, y) positions for the atoms.\n",
        "    \"\"\"\n",
        "    positions = []\n",
        "    x_center, y_center = center  # Center of the first triangle\n",
        "    height = (math.sqrt(3) / 2) * side_length  # Height of the equilateral triangle\n",
        "\n",
        "    # Generate complete triangles\n",
        "    num_full_triangles = N_a // 3\n",
        "    for triangle_idx in range(num_full_triangles):\n",
        "        # Calculate the center for the current triangle\n",
        "        current_x_center = x_center + triangle_idx * triangle_spacing_factor * side_length\n",
        "        current_y_center = y_center\n",
        "\n",
        "        # Place atoms at the corners of the equilateral triangle\n",
        "        positions.append((current_x_center - side_length / 2, current_y_center - height / 2))  # Bottom left\n",
        "        positions.append((current_x_center + side_length / 2, current_y_center - height / 2))  # Bottom right\n",
        "        positions.append((current_x_center, current_y_center + height / 2))  # Top vertex\n",
        "\n",
        "    # Place remaining atoms (if any)\n",
        "    remaining_atoms = N_a % 3\n",
        "    if remaining_atoms > 0:\n",
        "        # Get the last triangle's center\n",
        "        if num_full_triangles > 0:\n",
        "            last_x_center = x_center + (num_full_triangles - 1) * triangle_spacing_factor * side_length\n",
        "            last_y_center = y_center\n",
        "        else:\n",
        "            last_x_center = x_center\n",
        "            last_y_center = y_center\n",
        "\n",
        "        # Add remaining atoms sequentially around the last triangle\n",
        "        if remaining_atoms >= 1:\n",
        "            positions.append((last_x_center - side_length / 2, last_y_center - height / 2))  # Bottom left\n",
        "        if remaining_atoms == 2:\n",
        "            positions.append((last_x_center + side_length / 2, last_y_center - height / 2))  # Bottom right\n",
        "\n",
        "    return positions\n",
        "\n",
        "# Dipoles for each atom\n",
        "def generate_dipoles(N_a):\n",
        "    dipoles = [jnp.array([1.0, 0]) for _ in range(N_a)]\n",
        "    return dipoles\n",
        "\n",
        "# Function to generate SLM modulations\n",
        "def generate_slm_mod(N_slm):\n",
        "    phase_mod = jnp.zeros(N_slm)\n",
        "    amp_mod = jnp.ones(N_slm)\n",
        "    return phase_mod, amp_mod\n",
        "\n",
        "# Functions for crosstalk model setup (with fabrication variations considered)\n",
        "def generate_distances(num_channels, pitch, random_variation=0.0, seed=None):\n",
        "    \"\"\"\n",
        "    Generate pairwise distances for waveguides arranged in a linear layout with optional randomness.\n",
        "\n",
        "    Parameters:\n",
        "        num_channels (int): Number of waveguides.\n",
        "        pitch (float): Spacing between adjacent waveguides (µm).\n",
        "        random_variation (float): Maximum random variation (±) for distances.\n",
        "        seed (int or None): Seed for reproducibility of randomness.\n",
        "\n",
        "    Returns:\n",
        "        jax.numpy.array: Pairwise distances matrix (µm).\n",
        "    \"\"\"\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    distances = np.zeros((num_channels, num_channels), dtype=np.float32)\n",
        "    for i in range(num_channels):\n",
        "        for j in range(num_channels):\n",
        "            if i != j:\n",
        "                # Add randomness to the calculated distance\n",
        "                base_distance = abs(i - j) * pitch\n",
        "                variation = np.random.uniform(-random_variation, random_variation)\n",
        "                distances[i, j] = base_distance + variation\n",
        "    return jnp.array(distances)\n",
        "\n",
        "def generate_coupling_lengths(num_channels, base_length, scaling_factor=1.0, random_variation=0.0, seed=None):\n",
        "    \"\"\"\n",
        "    Generate coupling lengths with optional randomness.\n",
        "\n",
        "    Parameters:\n",
        "        num_channels (int): Number of waveguides.\n",
        "        base_length (float): Base coupling length for adjacent waveguides (µm).\n",
        "        scaling_factor (float): Multiplier to scale coupling length with increasing separation.\n",
        "        random_variation (float): Maximum random variation (±) for coupling lengths.\n",
        "        seed (int or None): Seed for reproducibility of randomness.\n",
        "\n",
        "    Returns:\n",
        "        jax.numpy.array: Coupling lengths matrix (µm).\n",
        "    \"\"\"\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    coupling_lengths = np.zeros((num_channels, num_channels), dtype=np.float32)\n",
        "    for i in range(num_channels):\n",
        "        for j in range(num_channels):\n",
        "            if i != j:\n",
        "                base_length_ij = base_length * (scaling_factor**abs(i - j))\n",
        "                variation = np.random.uniform(-random_variation, random_variation)\n",
        "                coupling_lengths[i, j] = base_length_ij + variation\n",
        "    return jnp.array(coupling_lengths)\n",
        "\n",
        "def generate_n_eff_list(num_channels, base_n_eff, random_variation=0.0, seed=None):\n",
        "    \"\"\"\n",
        "    Generate an array of effective refractive indices with optional randomness.\n",
        "\n",
        "    Parameters:\n",
        "        num_channels (int): Number of waveguides.\n",
        "        base_n_eff (float): Base effective refractive index for all waveguides.\n",
        "        random_variation (float): Maximum random variation (±) around the base value.\n",
        "        seed (int or None): Seed for reproducibility of randomness.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Array of effective refractive indices (n_eff_list).\n",
        "    \"\"\"\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    variations = np.random.uniform(-random_variation, random_variation, num_channels)\n",
        "    n_eff_list = base_n_eff + variations\n",
        "    return jnp.array(n_eff_list)\n",
        "\n",
        "\n",
        "# 1.1 Control Signal Construction\n",
        "\n",
        "def construct_V_smooth_with_carrier(tmin, tmax, t_steps, voltage_levels, omega_0, phi=0, max_step=30):\n",
        "    \"\"\"\n",
        "    Constructs a time-dependent control signal with piecewise smooth voltage levels and a carrier frequency component.\n",
        "\n",
        "    Parameters:\n",
        "    - tmin (float): Start time.\n",
        "    - tmax (float): End time.\n",
        "    - t_steps (int): Number of time steps.\n",
        "    - voltage_levels (array): Voltage levels for each piece.\n",
        "    - omega_0 (float): Carrier angular frequency (rad/s).\n",
        "    - phi (float, optional): Initial phase of the carrier (radians). Default is 0.\n",
        "    - max_step (float, optional): Maximum allowed voltage change between consecutive pieces. Default is 30.\n",
        "\n",
        "    Returns:\n",
        "    - V_t (array): Time-dependent control voltage with carrier modulation.\n",
        "    \"\"\"\n",
        "    time_points = jnp.linspace(tmin, tmax, t_steps)\n",
        "    num_pieces = len(voltage_levels)\n",
        "    piece_duration = t_steps // num_pieces\n",
        "\n",
        "    # Ensure voltage_levels is a JAX array\n",
        "    voltage_levels = jnp.array(voltage_levels, dtype=jnp.float32)\n",
        "\n",
        "    V_piecewise = jnp.zeros_like(time_points)\n",
        "\n",
        "    for i in range(num_pieces):\n",
        "        if i > 0:\n",
        "            delta_V = voltage_levels[i] - voltage_levels[i - 1]\n",
        "            if jnp.abs(delta_V) > max_step:\n",
        "                # Limit the voltage change to max_step\n",
        "                voltage_levels = voltage_levels.at[i].set(\n",
        "                    voltage_levels[i - 1] + jnp.sign(delta_V) * max_step\n",
        "                )\n",
        "\n",
        "        start_idx = i * piece_duration\n",
        "        # Ensure the last piece covers any remaining time steps\n",
        "        end_idx = (i + 1) * piece_duration if i < num_pieces - 1 else t_steps\n",
        "        V_piecewise = V_piecewise.at[start_idx:end_idx].set(\n",
        "            jnp.full(end_idx - start_idx, voltage_levels[i])\n",
        "        )\n",
        "\n",
        "    # Carrier modulation\n",
        "    carrier = jnp.cos(omega_0 * time_points + phi)\n",
        "    V_t = V_piecewise * carrier\n",
        "\n",
        "    return V_t\n",
        "\n",
        "\n",
        "# 1.2 Unitary Matrix Construction\n",
        "def dn(V):\n",
        "    return 4e-5 * V\n",
        "\n",
        "def phi_func(L, n, dn_val, lambda_):\n",
        "    return 2 * jnp.pi * L * (n + dn_val) / lambda_\n",
        "\n",
        "def D_func(a, t_val, phi_val):\n",
        "    return jnp.exp(1j * jnp.pi) * (a - t_val * jnp.exp(-1j * phi_val)) / (1 - t_val * a * jnp.exp(-1j * phi_val))\n",
        "\n",
        "def U_drmzm_single_channel(V0, V1, L, n0, lambda_0, a0, t0, a1, t1, psi_0=0):\n",
        "    dn_0 = dn(V0)\n",
        "    dn_1 = dn(V1)\n",
        "    phi0 = phi_func(L, n0, dn_0, lambda_0)\n",
        "    phi1 = phi_func(L, n0, dn_1, lambda_0)\n",
        "\n",
        "    D_00 = D_func(a0, t0, phi0) * jnp.exp(psi_0)\n",
        "    D_11 = D_func(a1, t1, phi1)\n",
        "\n",
        "    U_drmzm_matrix = jnp.array([[D_00, jnp.zeros_like(D_00)], [jnp.zeros_like(D_11), D_11]])\n",
        "    U_bs = (1 / jnp.sqrt(2)) * jnp.array([[1, 1], [1, -1]])\n",
        "    U_total = jnp.dot(U_bs, jnp.dot(U_drmzm_matrix, U_bs))\n",
        "\n",
        "    return U_total\n",
        "\n",
        "def U_drmzm_multi_channel(\n",
        "    V0_t_list, V1_t_list, L, n0, lambda_0, a0, t0, a1, t1,\n",
        "    N_ch, distances, coupling_lengths, n_eff_list, kappa0, alpha, enable_crosstalk=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Constructs the multi-channel unitary matrix with optional inter-channel coupling (amplitude and phase crosstalk).\n",
        "    No Python 'if distances[i,j] > 0:' is used; we use jnp.where to avoid TracerBoolConversionError.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Construct U_multi_channel_no_ct (diagonal only, no crosstalk)\n",
        "    U_multi_channel_no_ct = jnp.zeros((N_ch, N_ch), dtype=jnp.complex64)\n",
        "    for i in range(N_ch):\n",
        "        V0_t = V0_t_list[i]\n",
        "        V1_t = V1_t_list[i]\n",
        "        U_single_channel = U_drmzm_single_channel(\n",
        "            V0_t, V1_t, L, n0, lambda_0, a0, t0, a1, t1\n",
        "        )\n",
        "        # Just place the (0,0) element on the diagonal\n",
        "        U_multi_channel_no_ct = U_multi_channel_no_ct.at[i, i].set(U_single_channel[0, 0])\n",
        "\n",
        "    # If crosstalk is disabled, return the diagonal matrix only\n",
        "    if not enable_crosstalk:\n",
        "        return U_multi_channel_no_ct\n",
        "\n",
        "    # Step 2: Initialize crosstalk matrix as identity\n",
        "    U_wg_coupling_ct = jnp.eye(N_ch, dtype=jnp.complex64)\n",
        "\n",
        "    # Wave vector k = 2π / λ\n",
        "    k = (2.0 * jnp.pi) / lambda_0\n",
        "\n",
        "    def compute_transfer_matrix(L_val, beta_1, beta_2, kappa):\n",
        "        \"\"\"Compute transfer matrix for two coupled waveguides.\"\"\"\n",
        "        delta_beta = (beta_1 - beta_2) / 2.0\n",
        "        kappa_eff = jnp.sqrt(kappa**2 + delta_beta**2)  # Effective coupling\n",
        "        cos_term = jnp.cos(kappa_eff * L_val)\n",
        "        sin_term = jnp.sin(kappa_eff * L_val)\n",
        "        # Avoid Python 'if kappa_eff != 0:'; use a jnp.where if needed:\n",
        "        safe_kappa_eff = jnp.where(kappa_eff != 0, kappa_eff, 1e-12)  # small epsilon\n",
        "        delta_term = delta_beta / safe_kappa_eff\n",
        "\n",
        "        return jnp.array([\n",
        "            [cos_term - 1j * delta_term * sin_term, -1j * sin_term],\n",
        "            [-1j * sin_term, cos_term + 1j * delta_term * sin_term]\n",
        "        ], dtype=jnp.complex64)\n",
        "\n",
        "    # Step 3: Build crosstalk off-diagonal with jnp.where instead of Python if\n",
        "    for i in range(N_ch):\n",
        "        beta_i = k * n_eff_list[i]\n",
        "        for j in range(i + 1, N_ch):\n",
        "            beta_j = k * n_eff_list[j]\n",
        "\n",
        "            dist_ij = distances[i, j]                # distance\n",
        "            L_ij_full = coupling_lengths[i, j]       # nominal coupling length\n",
        "            # If dist_ij <= 0 => set them to 0.0 by jnp.where\n",
        "            L_ij = jnp.where(dist_ij > 0.0, L_ij_full, 0.0)\n",
        "            kappa_ij = jnp.where(dist_ij > 0.0,\n",
        "                                 kappa0 * jnp.exp(-alpha * dist_ij),\n",
        "                                 0.0)\n",
        "\n",
        "            # Compute transfer matrix\n",
        "            M = compute_transfer_matrix(L_ij, beta_i, beta_j, kappa_ij)\n",
        "\n",
        "            # Update crosstalk matrix with amplitude and phase contributions\n",
        "            # (0,1) element goes in [i,j], conj(...) goes in [j,i]\n",
        "            U_wg_coupling_ct = U_wg_coupling_ct.at[i, j].set(M[0, 1])\n",
        "            U_wg_coupling_ct = U_wg_coupling_ct.at[j, i].set(jnp.conj(M[0, 1]))\n",
        "\n",
        "    # Step 4: Combine diagonal matrix (no crosstalk) with crosstalk matrix\n",
        "    U_multi_channel_with_ct = jnp.matmul(U_multi_channel_no_ct, U_wg_coupling_ct)\n",
        "\n",
        "    return U_multi_channel_with_ct\n",
        "\n",
        "# 1.3 SLM and Scattering Matrices\n",
        "def construct_U_multi_channel_slm(N_slm, phase_mod, amp_mod, t_steps):\n",
        "    U_slm = jnp.zeros((t_steps, N_slm, N_slm), dtype=jnp.complex64)\n",
        "    for t in range(t_steps):\n",
        "        for i in range(N_slm):\n",
        "            phase = jnp.exp(1j * phase_mod[i])\n",
        "            amplitude = amp_mod[i]\n",
        "            U_slm = U_slm.at[t, i, i].set(amplitude * phase)\n",
        "    return U_slm\n",
        "\n",
        "def construct_I_prime(N_scat, delta, t_steps):\n",
        "    I_prime = jnp.zeros((t_steps, N_scat, N_scat), dtype=jnp.complex64)\n",
        "    for t in range(t_steps):\n",
        "        I_prime = I_prime.at[t].set(jnp.eye(N_scat) + jnp.diag(jnp.full(N_scat, delta)))\n",
        "    return I_prime\n",
        "\n",
        "# 1.4 E-field Calculations\n",
        "def lg00_mode_profile(X, Y, beam_center, beam_waist):\n",
        "    cx, cy = beam_center\n",
        "    r_squared = (X - cx)**2 + (Y - cy)**2\n",
        "    E_profile = jnp.exp(-r_squared / (2 * beam_waist**2))\n",
        "    return E_profile\n",
        "\n",
        "def compute_E_field_for_channel(X, Y, E_t, beam_center, beam_waist, t_idx):\n",
        "    E_profile = lg00_mode_profile(X, Y, beam_center, beam_waist)\n",
        "    E_field = E_profile * (jnp.real(E_t) + 1j * jnp.imag(E_t))  # Corrected indexing\n",
        "    return E_field\n",
        "\n",
        "def compute_total_E_field_profile(X, Y, b_slm_out, beam_centers, beam_waist):\n",
        "    E_field_profiles = []\n",
        "    for t_idx in range(b_slm_out.shape[0]):\n",
        "        E_field_total = jnp.zeros_like(X, dtype=jnp.complex64)\n",
        "        for atom_index, beam_center in enumerate(beam_centers):\n",
        "            E_field_total += compute_E_field_for_channel(X, Y, b_slm_out[t_idx, atom_index], beam_center, beam_waist, t_idx)\n",
        "        E_field_profiles.append(E_field_total)\n",
        "    return jnp.array(E_field_profiles)\n",
        "\n",
        "def extract_E_field_at_atoms(E_field_profiles, atom_positions, X, Y):\n",
        "    E_field_at_atoms = []\n",
        "\n",
        "    for t_idx in range(E_field_profiles.shape[0]):\n",
        "        E_field_at_timestep = []\n",
        "        for x0, y0 in atom_positions:\n",
        "            x_idx = jnp.argmin(jnp.abs(X[0, :] - x0))\n",
        "            y_idx = jnp.argmin(jnp.abs(Y[:, 0] - y0))\n",
        "            E_field_at_timestep.append(E_field_profiles[t_idx][y_idx, x_idx])\n",
        "        E_field_at_atoms.append(jnp.array(E_field_at_timestep))\n",
        "    return jnp.array(E_field_at_atoms)\n",
        "\n",
        "def compute_alpha_t(E_fields_at_atoms, dipoles, Omega_prefactor_MHz):\n",
        "    alpha_t = []\n",
        "    for t_idx in range(E_fields_at_atoms.shape[0]):\n",
        "        alpha_t_timestep = []\n",
        "        for atom_idx in range(len(dipoles)):\n",
        "            E_field_atom = E_fields_at_atoms[t_idx, atom_idx]\n",
        "            dipole = dipoles[atom_idx]\n",
        "            alpha_t_timestep.append(Omega_prefactor_MHz * dipole[0] * E_field_atom)\n",
        "        alpha_t.append(jnp.array(alpha_t_timestep))\n",
        "    return jnp.array(alpha_t)\n",
        "\n",
        "\n",
        "# ========================================\n",
        "# Part 2: Atomic Model\n",
        "# ========================================\n",
        "\n",
        "# Two levels per qubit\n",
        "N_qubit_level = 2\n",
        "\n",
        "# Define multi-qubit operators and Hamiltonians (as described previously)\n",
        "s_plus = jnp.array([[0, 1], [0, 0]], dtype=jnp.complex64)\n",
        "s_minus = jnp.array([[0, 0], [1, 0]], dtype=jnp.complex64)\n",
        "s_z = jnp.array([[1, 0], [0, -1]], dtype=jnp.complex64)\n",
        "\n",
        "def construct_multi_qubit_operator(single_qubit_op, N_a, N_qubit_level, qubit_idx):\n",
        "    I = jnp.eye(N_qubit_level, dtype=jnp.complex64)\n",
        "    op_list = [I] * N_a\n",
        "    op_list[qubit_idx] = single_qubit_op\n",
        "\n",
        "    multi_qubit_op = op_list[0]\n",
        "    for op in op_list[1:]:\n",
        "        multi_qubit_op = jnp.kron(multi_qubit_op, op)\n",
        "\n",
        "    return multi_qubit_op\n",
        "\n",
        "# Creation and annihilation operators for the quantized field (Fock mode)\n",
        "def construct_annihilation_operator(fock_dim):\n",
        "    a = jnp.zeros((fock_dim, fock_dim), dtype=jnp.complex64)\n",
        "    for n in range(1, fock_dim):\n",
        "        a = a.at[n - 1, n].set(jnp.sqrt(n))\n",
        "    a_dag = a.T.conj()  # Creation operator is the Hermitian conjugate of annihilation operator\n",
        "    return a, a_dag\n",
        "\n",
        "# Fock space dimensions\n",
        "fock_dim = 5\n",
        "a, a_dag = construct_annihilation_operator(fock_dim)\n",
        "I_fock = jnp.eye(fock_dim, dtype=jnp.complex64)\n",
        "\n",
        "# Drift Hamiltonian (H_0)\n",
        "def construct_H_0(N_a, omega_0, omega_r):  # omega_0/r in MHz\n",
        "    H_0_qubits = sum(0.5 * omega_0 * construct_multi_qubit_operator(s_z, N_a, 2, i) for i in range(N_a))\n",
        "    H_0_field = omega_r * jnp.kron(jnp.eye(2 ** N_a), a_dag @ a)\n",
        "    return jnp.kron(H_0_qubits, I_fock) + H_0_field\n",
        "\n",
        "# Control Hamiltonian\n",
        "def construct_H_control(N_a, N_qubit_level, g_real_t, g_imag_t):\n",
        "    H_control = jnp.zeros((N_qubit_level ** N_a * fock_dim, N_qubit_level ** N_a * fock_dim), dtype=jnp.complex64)\n",
        "    for i in range(N_a):\n",
        "        H_control += g_real_t[i] * (jnp.kron(construct_multi_qubit_operator(s_plus, N_a, N_qubit_level, i), a) +\n",
        "                                    jnp.kron(construct_multi_qubit_operator(s_minus, N_a, N_qubit_level, i), a_dag))\n",
        "        H_control += g_imag_t[i] * (1j * jnp.kron(construct_multi_qubit_operator(s_plus, N_a, N_qubit_level, i), a) -\n",
        "                                    1j * jnp.kron(construct_multi_qubit_operator(s_minus, N_a, N_qubit_level, i), a_dag))\n",
        "    return H_control\n",
        "\n",
        "# Time Evolution Hamiltonian (H(t))\n",
        "def construct_H_time(N_a, N_qubit_level, omega_0, omega_r, g_real_t, g_imag_t,\n",
        "                   atom_positions, gate_type='single'):\n",
        "    \"\"\"Construct the time-dependent Hamiltonian H(t) as a JAX array.\"\"\"\n",
        "    H_t_list = []  # Store Hamiltonians for each time step\n",
        "\n",
        "    # Construct base Hamiltonians\n",
        "    H_0 = construct_H_0(N_a, omega_0, omega_r)\n",
        "\n",
        "    # Construct H(t) for each time step\n",
        "    for t in range(len(g_real_t)):\n",
        "        H_control = construct_H_control(N_a, N_qubit_level, g_real_t[t], g_imag_t[t])\n",
        "        H_t = H_0 + H_control\n",
        "        H_t_list.append(H_t)\n",
        "\n",
        "    # Convert list of Hamiltonians to a JAX array\n",
        "    return jnp.array(H_t_list)\n",
        "\n",
        "# Compute accumulated propagator (U(t)) over time\n",
        "def compute_accumulated_propagator(H_t, dt, N_a, N_qubit_level):\n",
        "    dim = N_qubit_level ** N_a * fock_dim\n",
        "    U_accumulated = jnp.eye(dim, dtype=jnp.complex64)\n",
        "    U_t_all = []\n",
        "    for H in H_t:\n",
        "        U_t = jla.expm(-1j * H * dt)\n",
        "        U_accumulated = U_t @ U_accumulated\n",
        "        U_t_all.append(U_accumulated)\n",
        "    return jnp.array(U_t_all)\n",
        "\n",
        "# Trace out field from unitary matrix\n",
        "def trace_out_field_from_unitary(U_t, N_a, N_qubit_level, field_dim=5):\n",
        "    dim_qubits = N_qubit_level ** N_a\n",
        "    U_t_reshaped = U_t.reshape(dim_qubits, field_dim, dim_qubits, field_dim)\n",
        "    U_t_traced = jnp.sum(U_t_reshaped, axis=(1, 3))\n",
        "    return U_t_traced\n",
        "\n",
        "def trace_out_field_from_unitary_t_all(U_t_all, N_a, N_qubit_level, field_dim=5):\n",
        "    \"\"\"\n",
        "    Trace out the field from the unitary evolution matrix for all time steps.\n",
        "\n",
        "    Parameters:\n",
        "    - U_t_all: Time series of unitary matrices with shape (time_steps, dim_total, dim_total),\n",
        "      where dim_total = dim_qubits * field_dim.\n",
        "    - N_a: Number of atoms (qubits).\n",
        "    - N_qubit_level: Number of levels per qubit (typically 2).\n",
        "    - field_dim: Dimension of the field Hilbert space.\n",
        "\n",
        "    Returns:\n",
        "    - U_t_traced_all: Time series of unitary matrices with field traced out, shape (time_steps, dim_qubits, dim_qubits).\n",
        "    \"\"\"\n",
        "    time_steps = U_t_all.shape[0]\n",
        "    dim_qubits = N_qubit_level**N_a\n",
        "    total_dim = U_t_all.shape[-1]\n",
        "\n",
        "    # Ensure dimensions match\n",
        "    assert dim_qubits * field_dim == total_dim, \"Mismatch in total dimensions!\"\n",
        "\n",
        "    # Reshape to separate qubit and field dimensions\n",
        "    U_t_reshaped = U_t_all.reshape(time_steps, dim_qubits, field_dim, dim_qubits, field_dim)\n",
        "\n",
        "    # Trace out field dimensions\n",
        "    U_t_traced_all = jnp.sum(U_t_reshaped, axis=(2, 4))  # Sum over field dimensions\n",
        "\n",
        "    return U_t_traced_all\n",
        "\n",
        "# Fidelity Calculation\n",
        "def compute_fidelity_unitary(U_t_traced, U_target):\n",
        "    \"\"\"\n",
        "    Computes the fidelity of the unitary evolution compared to the target unitary\n",
        "    using the standard multi-qubit gate fidelity formula.\n",
        "\n",
        "    Parameters:\n",
        "    - U_t_traced: Reduced unitary matrix for the qubit system after tracing out the field.\n",
        "    - U_target: Target unitary matrix.\n",
        "\n",
        "    Returns:\n",
        "    - fidelity: The fidelity between the evolved and target unitary matrices.\n",
        "    \"\"\"\n",
        "    # Dimension of the Hilbert space\n",
        "    d = U_target.shape[0]  # For N qubits, d = 2^N\n",
        "\n",
        "    # Compute fidelity using the standard formula\n",
        "    trace_overlap = jnp.abs(jnp.trace(U_target.conj().T @ U_t_traced))**2\n",
        "    fidelity = trace_overlap / (d**2)\n",
        "\n",
        "    return jnp.real(fidelity)\n",
        "\n",
        "# record F(t)\n",
        "def compute_fidelity_unitary_t_all(U_t_traced_all, U_target):\n",
        "    \"\"\"\n",
        "    Computes the fidelity of the unitary evolution for all time steps\n",
        "    compared to the target unitary using the standard multi-qubit gate fidelity formula.\n",
        "\n",
        "    Parameters:\n",
        "    - U_t_traced_all: A list or array of reduced unitary matrices for the qubit system at all time steps.\n",
        "    - U_target: Target unitary matrix.\n",
        "\n",
        "    Returns:\n",
        "    - fidelity_all: Array of fidelities at each time step.\n",
        "    \"\"\"\n",
        "    # Dimension of the Hilbert space\n",
        "    d = U_target.shape[0]  # For N qubits, d = 2^N\n",
        "\n",
        "    def fidelity_at_timestep(U_t_traced):\n",
        "        trace_overlap = jnp.abs(jnp.trace(U_target.conj().T @ U_t_traced))**2\n",
        "        return trace_overlap / (d**2)\n",
        "\n",
        "    # Vectorized computation of fidelity for all time steps\n",
        "    fidelity_all = jax.vmap(fidelity_at_timestep)(U_t_traced_all)\n",
        "    return fidelity_all\n",
        "\n",
        "\n",
        "# Clifford gates\n",
        "def clifford_group_and_t_gate():\n",
        "    \"\"\"\n",
        "    Constructs the single-qubit Clifford group and includes the T gate.\n",
        "\n",
        "    Returns:\n",
        "    - clifford_group: List of tuples containing the gate name and matrix.\n",
        "    \"\"\"\n",
        "    # Define single-qubit gates\n",
        "    I = jnp.array([[1, 0], [0, 1]], dtype=jnp.complex64)  # Identity\n",
        "    H = (1 / jnp.sqrt(2)) * jnp.array([[1, 1], [1, -1]], dtype=jnp.complex64)  # Hadamard\n",
        "    S = jnp.array([[1, 0], [0, 1j]], dtype=jnp.complex64)  # Phase gate\n",
        "    X = jnp.array([[0, 1], [1, 0]], dtype=jnp.complex64)  # Pauli-X\n",
        "    Y = jnp.array([[0, -1j], [1j, 0]], dtype=jnp.complex64)  # Pauli-Y\n",
        "    Z = jnp.array([[1, 0], [0, -1]], dtype=jnp.complex64)  # Pauli-Z\n",
        "    T = jnp.array([[1, 0], [0, jnp.exp(1j * jnp.pi / 4)]], dtype=jnp.complex64)  # T gate (π/8 rotation)\n",
        "\n",
        "    # Clifford group: combinations of I, H, S, X, Y, Z\n",
        "    # The single-qubit Clifford group consists of 24 elements.\n",
        "    clifford_group = [\n",
        "        (\"X\", X),\n",
        "        (\"I\", I),\n",
        "        (\"H\", H),\n",
        "        (\"S\", S),\n",
        "        (\"Y\", Y),\n",
        "        (\"Z\", Z),\n",
        "        (\"HS\", H @ S),\n",
        "        (\"SH\", S @ H),\n",
        "        (\"HX\", H @ X),\n",
        "        (\"SX\", S @ X),\n",
        "        (\"SY\", S @ Y),\n",
        "        (\"SZ\", S @ Z),\n",
        "        (\"XH\", X @ H),\n",
        "        (\"YH\", Y @ H),\n",
        "        (\"ZH\", Z @ H),\n",
        "        (\"XS\", X @ S),\n",
        "        (\"YS\", Y @ S),\n",
        "        (\"ZS\", Z @ S),\n",
        "        (\"HSX\", H @ S @ X),\n",
        "        (\"HSY\", H @ S @ Y),\n",
        "        (\"HSZ\", H @ S @ Z),\n",
        "        (\"XHS\", X @ H @ S),\n",
        "        (\"YHS\", Y @ H @ S),\n",
        "        (\"ZHS\", Z @ H @ S),\n",
        "    ]\n",
        "\n",
        "    # Include T gate for extended functionality\n",
        "    clifford_group_with_t = clifford_group + [(\"T\", T)]\n",
        "\n",
        "    return clifford_group_with_t\n",
        "\n",
        "\n",
        "# ========================================\n",
        "# Part 3: Function to Compute Gate Fidelity\n",
        "# ========================================\n",
        "\n",
        "def compute_multi_qubit_fidelity_closed_system(\n",
        "    V0_t_list, V1_t_list, L, n0, lambda_0, a0, t0, a1, t1, phase_mod, amp_mod, delta,\n",
        "    atom_positions, dipoles, beam_centers, beam_waist, X, Y, Omega_prefactor_MHz,\n",
        "    t_steps, dt,\n",
        "    N_ch, distances, coupling_lengths, n_eff_list, kappa0, alpha, enable_crosstalk,\n",
        "    N_slm, N_ch_slm_in, N_scat_1, N_scat_2, N_a, N_qubit_level, omega_0, omega_r, a_pic, a_scat_1,\n",
        "    U_target, gate_type='single'\n",
        "):\n",
        "    \"\"\"\n",
        "    Computes the multi-qubit fidelity for a closed system with a fixed tmax.\n",
        "\n",
        "    Parameters:\n",
        "        V0_t_list, V1_t_list: Control voltage time-series for each channel.\n",
        "        L, n0, lambda_0, a0, t0, a1, t1: Physical parameters of the MZM.\n",
        "        phase_mod, amp_mod: Phase and amplitude modulation parameters.\n",
        "        delta: Detuning.\n",
        "        atom_positions, dipoles: Atom positions and dipole moments.\n",
        "        beam_centers, beam_waist: Parameters of the spatial light modulator (SLM).\n",
        "        X, Y: 2D grid for the beam field.\n",
        "        Omega_prefactor_MHz: Prefactor for Rabi frequency in MHz.\n",
        "        t_steps, dt: Time parameters.\n",
        "        N_ch: Number of photonic channels.\n",
        "        distances: Pairwise distances between channels (µm).\n",
        "        coupling_lengths: Pairwise coupling lengths (µm).\n",
        "        n_eff_list: Effective refractive indices for the waveguides.\n",
        "        kappa0, alpha: Crosstalk parameters.\n",
        "        enable_crosstalk (bool): If False, crosstalk is disabled.\n",
        "        N_slm, N_ch_slm_in, N_scat_1, N_scat_2, N_a, N_qubit_level: System configuration.\n",
        "        omega_0, omega_r: Frequencies in MHz.\n",
        "        a_pic, a_scat_1: Initial fields.\n",
        "        U_target: Target unitary matrix.\n",
        "        gate_type: Type of quantum gate ('single', 'multi', etc.).\n",
        "\n",
        "    Returns:\n",
        "        Fidelity of the computed unitary matrix with respect to the target matrix.\n",
        "    \"\"\"\n",
        "    # Compute Unitary Matrix for Multi-Channel System\n",
        "    U_system_multi_channel = jnp.array([\n",
        "        U_drmzm_multi_channel(\n",
        "            [V0_t[i] for V0_t in V0_t_list],\n",
        "            [V1_t[i] for V1_t in V1_t_list],\n",
        "            L, n0, lambda_0, a0, t0, a1, t1,\n",
        "            N_ch, distances, coupling_lengths, n_eff_list,\n",
        "            kappa0, alpha, enable_crosstalk\n",
        "        )\n",
        "        for i in range(t_steps)\n",
        "    ])\n",
        "\n",
        "    # Compute scattering and SLM matrices\n",
        "    I1_prime = construct_I_prime(N_scat_1, delta, t_steps)\n",
        "    U_tensor_product = jnp.array([jnp.kron(U_system_multi_channel[i], I1_prime[i]) for i in range(t_steps)])\n",
        "\n",
        "    # Output modes calculations\n",
        "    a_total = jnp.kron(a_pic, a_scat_1)\n",
        "    output_modes = jnp.array([jnp.dot(U_tensor_product[i], a_total) for i in range(t_steps)])\n",
        "    output_modes_reshaped = output_modes.reshape(t_steps, N_ch, N_scat_1)\n",
        "    a_pic_out = jnp.sum(output_modes_reshaped, axis=-1)\n",
        "    a_scat_1_out = jnp.sum(output_modes_reshaped, axis=-2)\n",
        "\n",
        "    # Compute final output modes after SLM and second scattering\n",
        "    b_slm_in = jnp.zeros((t_steps, N_slm), dtype=jnp.complex64)\n",
        "    b_scat2_in = jnp.zeros((t_steps, N_scat_2), dtype=jnp.complex64)\n",
        "\n",
        "    for t in range(t_steps):\n",
        "        b_slm_in = b_slm_in.at[t, :N_ch_slm_in].set(a_pic_out[t, :N_ch_slm_in])\n",
        "        b_slm_in = b_slm_in.at[t, N_ch_slm_in:].set(a_scat_1_out[t, :(N_slm - N_ch_slm_in)])\n",
        "\n",
        "        b_scat2_in = b_scat2_in.at[t, :(N_ch - N_ch_slm_in)].set(a_pic_out[t, N_ch_slm_in:])\n",
        "        b_scat2_in = b_scat2_in.at[t, (N_ch - N_ch_slm_in):].set(a_scat_1_out[t, (N_scat_2 - (N_ch - N_ch_slm_in)):])\n",
        "\n",
        "    b_total_in = jnp.array([jnp.kron(b_slm_in[t], b_scat2_in[t]) for t in range(t_steps)])\n",
        "    U_multi_channel_slm = construct_U_multi_channel_slm(N_slm, phase_mod, amp_mod, t_steps)\n",
        "    I2_prime = construct_I_prime(N_scat_2, delta, t_steps)\n",
        "    U_tensor_product_stage_2 = jnp.array([jnp.kron(U_multi_channel_slm[t], I2_prime[t]) for t in range(t_steps)])\n",
        "    b_total_out = jnp.array([jnp.dot(U_tensor_product_stage_2[t], b_total_in[t]) for t in range(t_steps)])\n",
        "\n",
        "    b_total_out_reshaped = b_total_out.reshape(t_steps, N_slm, N_scat_2)\n",
        "    b_slm_out = jnp.sum(b_total_out_reshaped, axis=-1)\n",
        "\n",
        "    # Compute the total E-field on the atom plane\n",
        "    E_field_profiles = compute_total_E_field_profile(X, Y, b_slm_out, beam_centers, beam_waist)\n",
        "\n",
        "    # Extract E-fields at the atom positions\n",
        "    E_fields_at_atoms = extract_E_field_at_atoms(E_field_profiles, atom_positions, X, Y)\n",
        "\n",
        "    # Compute interaction strength α(t) for each atom\n",
        "    alpha_t = compute_alpha_t(E_fields_at_atoms, dipoles, Omega_prefactor_MHz)\n",
        "\n",
        "    # g_real_t and g_imag_t represent g(t) in the Jaynes-Cummings model, g(t) = eff_Rabi(t)/2\n",
        "    g_real_t = alpha_t.real / 2\n",
        "    g_imag_t = alpha_t.imag / 2\n",
        "\n",
        "    # Construct the time-dependent Hamiltonian\n",
        "    H_t = construct_H_time(N_a, N_qubit_level, omega_0, omega_r, g_real_t, g_imag_t, atom_positions, gate_type)\n",
        "\n",
        "    # Evolve the unitary matrix (propagator)\n",
        "    U_t_all = compute_accumulated_propagator(H_t, dt, N_a, N_qubit_level)\n",
        "\n",
        "    # Trace out the field from the final unitary matrix\n",
        "    U_t_traced = trace_out_field_from_unitary(U_t_all[-1], N_a, N_qubit_level)\n",
        "    U_t_traced_all = trace_out_field_from_unitary_t_all(U_t_all, N_a, N_qubit_level)\n",
        "\n",
        "    # Compute fidelity with the target gate\n",
        "    fidelity = compute_fidelity_unitary(U_t_traced, U_target)\n",
        "    fidelity_all = compute_fidelity_unitary_t_all(U_t_traced_all, U_target)\n",
        "\n",
        "    return fidelity_all\n",
        "\n",
        "# ========================================\n",
        "# Part 4: Program Instruction function\n",
        "# ========================================\n",
        "def program_instruction(N_a, key_number, gate_type='single', selected_atoms=None, control_atom=None, target_atom=None):\n",
        "    \"\"\"\n",
        "    Constructs the target gates for selected atoms. If `gate_type` is 'single', it applies single-qubit gates.\n",
        "    Logs the gate applied to each atom.\n",
        "    \"\"\"\n",
        "    if gate_type == 'single':\n",
        "        # Define the Clifford gates for single-qubit gates\n",
        "        clifford_t_gates = clifford_group_and_t_gate()\n",
        "        U_target_multi_qubit = []\n",
        "        key = jrandom.PRNGKey(key_number)  # Initialize the random key for gate selection\n",
        "\n",
        "        # Apply random single-qubit gates to selected atoms\n",
        "        for atom_idx in range(N_a):\n",
        "            if selected_atoms is None or atom_idx in selected_atoms:\n",
        "                key, subkey = jrandom.split(key)  # Split key for each atom to ensure different gates\n",
        "                gate_idx = jrandom.randint(subkey, (), minval=0, maxval=len(clifford_t_gates))\n",
        "                gate_name, gate = clifford_t_gates[gate_idx]\n",
        "                logger.info(f\"Applying {gate_name} gate to Atom {atom_idx}\")\n",
        "                U_target_multi_qubit.append(gate)\n",
        "            else:\n",
        "                logger.info(f\"Applying Identity gate to Atom {atom_idx}\")\n",
        "                U_target_multi_qubit.append(jnp.eye(2))  # Identity gate for unselected atoms\n",
        "\n",
        "        # Combine the gates using the Kronecker product\n",
        "        U_target = U_target_multi_qubit[0]\n",
        "        for gate in U_target_multi_qubit[1:]:\n",
        "            U_target = jnp.kron(U_target, gate)\n",
        "\n",
        "        return U_target\n",
        "\n",
        "# ========================================\n",
        "# Part 5: Modified Configuration Setup\n",
        "# ========================================\n",
        "def create_config(simple=False):\n",
        "    # Define APIC parameters\n",
        "    APIC_params = {\n",
        "        'L': 600 * 780e-9 / 1.95,  # Example calculation based on your main code\n",
        "        'n0': 1.95,\n",
        "        'lambda_0': 780e-9,\n",
        "        'a0': 0.998,\n",
        "        't0': 0.998,\n",
        "        'a1': 0.998,\n",
        "        't1': 0.998,\n",
        "        'phase_mod': jnp.zeros(6),  # Assuming N_slm = 6\n",
        "        'amp_mod': jnp.ones(6)      # Assuming N_slm = 6\n",
        "    }\n",
        "\n",
        "    # Define atom beam parameters\n",
        "    X, Y = generate_grid(grid_size=600, grid_range=(-6, 6))\n",
        "    atom_positions = generate_atom_positions_equilateral(3, side_length=3.0, center=(0, 0))  # N_a = 3\n",
        "    dipoles = generate_dipoles(3)  # N_a = 3\n",
        "    beam_centers = atom_positions\n",
        "    beam_waist = 2.0  # µm\n",
        "    Omega_prefactor_MHz = calculate_Omega_rabi_prefactor(20, 1000)  # I_mW_per_cm2=20, Detuning_MHz=1000\n",
        "    a_pic = jnp.array([1.0] * 6)      # N_ch = 6\n",
        "    a_scat_1 = jnp.array([1.0] * 4)   # N_scat_1 = 4\n",
        "\n",
        "    atom_beam_params = {\n",
        "        'atom_positions': atom_positions,\n",
        "        'dipoles': dipoles,\n",
        "        'beam_centers': beam_centers,\n",
        "        'beam_waist': beam_waist,\n",
        "        'X': X,\n",
        "        'Y': Y,\n",
        "        'Omega_prefactor_MHz': Omega_prefactor_MHz,\n",
        "    }\n",
        "\n",
        "    # Define control voltage parameters\n",
        "    t_steps = 10  # **Changed** to align with the first training phase\n",
        "    tmax_fixed = 0.1  # Fixed tmax in microseconds\n",
        "    dt = tmax_fixed / t_steps\n",
        "    control_Vt_params = {\n",
        "        'tmin': 0.0,\n",
        "        'tmax': tmax_fixed,\n",
        "        't_steps': t_steps,\n",
        "        'dt': dt,\n",
        "        'min_V_level': -15.0,\n",
        "        'max_V_level': 15.0\n",
        "    }\n",
        "\n",
        "    # Define system parameters\n",
        "    system_params = {\n",
        "        'a_pic': a_pic,\n",
        "        'a_scat_1': a_scat_1,\n",
        "        'delta': 0.001,\n",
        "        'N_ch': 6,\n",
        "        'distances': generate_distances(6, 1.0, random_variation=0.1, seed=42),\n",
        "        'coupling_lengths': generate_coupling_lengths(6, 600.0, scaling_factor=1.1, random_variation=60.0, seed=42),\n",
        "        'n_eff_list': generate_n_eff_list(6, 1.75, random_variation=0.05, seed=42),\n",
        "        'kappa0': 10.145,\n",
        "        'alpha': 6.934,\n",
        "        'enable_crosstalk': True,\n",
        "        'N_slm': 6,\n",
        "        'N_ch_slm_in': 4,\n",
        "        'N_scat_1': 4,\n",
        "        'N_scat_2': 6 + 4 - 6,  # N_total - N_slm\n",
        "        'N_a': 3,\n",
        "        'N_qubit_level': 2,\n",
        "        'omega_0': 6.835e3,  # MHz\n",
        "        'omega_r': 6.835e3,  # MHz\n",
        "        'a_pic': a_pic,\n",
        "        'a_scat_1': a_scat_1,\n",
        "        'gate_type': 'single'\n",
        "    }\n",
        "\n",
        "    # Generate the target unitary\n",
        "    selected_atoms = [0,1,2]  # Set to [1, 2] as per N_a = 3\n",
        "    system_params['U_target'] = program_instruction(\n",
        "        N_a=3,\n",
        "        key_number=105,\n",
        "        gate_type='single',\n",
        "        selected_atoms=selected_atoms\n",
        "    )\n",
        "\n",
        "    # Defensive check to ensure U_target is not None\n",
        "    if system_params['U_target'] is None:\n",
        "        raise ValueError(\"U_target was not correctly assigned by program_instruction.\")\n",
        "\n",
        "    # Define reward scaling parameters with hybrid scaling methods\n",
        "    reward_scaling_params = {\n",
        "        'a': 1.0,  # Scaling factor for absolute fidelity\n",
        "        'b': 1.0,  # Scaling factor for fidelity improvement\n",
        "        'scaling_methods': [  # Define scaling methods with relative thresholds\n",
        "            {'method': 'log', 'min_ratio': 0.0, 'max_ratio': 0.5},\n",
        "            {'method': 'linear', 'min_ratio': 0.5, 'max_ratio': 0.9},\n",
        "            {'method': 'quadratic', 'min_ratio': 0.9, 'max_ratio': 1.0},\n",
        "        ],\n",
        "        'k': 5.0,  # Relevant for exponential scaling\n",
        "        'thresholds': [0.8, 0.9],  # Fidelity thresholds for bonuses (can also be made relative)\n",
        "        'bonus': 5.0,  # Bonus reward for crossing thresholds\n",
        "        'clip_min': -10.0,  # Minimum reward\n",
        "        'clip_max': 10.0,  # Maximum reward\n",
        "        'step_penalty': 0.01  # Penalty per step\n",
        "    }\n",
        "\n",
        "    config = {\n",
        "        't_steps': t_steps,\n",
        "        'N_ch': 6,\n",
        "        'N_a': 3,\n",
        "        'piecewise_segments': 10,  # **Changed** from 50 to 10 to align with the first training phase\n",
        "        'min_voltage': -15.0,\n",
        "        'max_voltage': 15.0,\n",
        "        'history_length': 5,  # Number of past states to include\n",
        "        'max_delta_voltage': 1.0,  # Added parameter for action scaling\n",
        "        'APIC_params': APIC_params,          # Unchanged\n",
        "        'system_params': system_params,      # Only system-related parameters\n",
        "        'atom_beam_params': atom_beam_params,\n",
        "        'control_Vt_params': control_Vt_params,\n",
        "        'reward_scaling_params': reward_scaling_params,  # Updated reward scaling parameters\n",
        "        'stagnant_threshold': 10,           # Number of stagnant episodes before termination\n",
        "        'stagnant_fidelity_min': 0.99,      # Minimum fidelity to consider stagnation\n",
        "        'target_fidelity': 0.999            # Desired fidelity threshold to achieve\n",
        "    }\n",
        "\n",
        "    return config\n",
        "\n",
        "# ========================================\n",
        "# Part 7: Modified Custom Callback (With Additional Logging)\n",
        "# ========================================\n",
        "class RLLoggingCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Custom callback for logging additional metrics during PPO training.\n",
        "    Logs to both console and a log file.\n",
        "    Implements early termination if best_fidelity is stuck between stagnant_fidelity_min and target_fidelity for too many episodes.\n",
        "    Also saves the final optimal V(t) and plots upon termination.\n",
        "    \"\"\"\n",
        "    def __init__(self, stagnant_threshold=100,\n",
        "                 stagnant_fidelity_min=0.99,\n",
        "                 target_fidelity=0.999,\n",
        "                 results_dir='results',\n",
        "                 verbose=0):\n",
        "        super(RLLoggingCallback, self).__init__(verbose)\n",
        "        self.best_fidelity = 0.0\n",
        "        self.best_voltage = None\n",
        "        self.best_fidelity_history = []\n",
        "        self.episode_count = 0\n",
        "        self.stagnant_counter = 0\n",
        "        self.stagnant_threshold = stagnant_threshold\n",
        "        self.stagnant_fidelity_min = stagnant_fidelity_min\n",
        "        self.target_fidelity = target_fidelity\n",
        "        self.start_time = time.perf_counter()\n",
        "        self.results_dir = results_dir\n",
        "\n",
        "        os.makedirs(self.results_dir, exist_ok=True)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        infos = self.locals.get('infos', [])\n",
        "        improved = False\n",
        "\n",
        "        # Additional log line for debugging how often this is called\n",
        "        logger.info(f\"[RLLoggingCallback] _on_step called. Infos count={len(infos)}. Current best fidelity={self.best_fidelity:.4f}\")\n",
        "\n",
        "        for env_id, info in enumerate(infos):\n",
        "            if 'final_fidelity' in info:\n",
        "                fidelity = info['final_fidelity']\n",
        "                self.episode_count += 1\n",
        "\n",
        "                if fidelity < 0.0 or fidelity > 1.0:\n",
        "                    continue  \n",
        "\n",
        "                logger.info(f\"[RLLoggingCallback] Env={env_id}, Episode={self.episode_count}, final_fidelity={fidelity:.6f}\")\n",
        "\n",
        "                # Check improvement\n",
        "                if fidelity > self.best_fidelity:\n",
        "                    self.best_fidelity = fidelity\n",
        "                    self.stagnant_counter = 0\n",
        "                    improved = True\n",
        "                    new_best_voltage = info.get('best_voltage', None)\n",
        "                    if new_best_voltage is not None:\n",
        "                        self.best_voltage = new_best_voltage\n",
        "\n",
        "                    logger.info(\n",
        "                        f\"New best fidelity: {self.best_fidelity:.6f} achieved in Env {env_id} | Episode: {self.episode_count}\"\n",
        "                    )\n",
        "\n",
        "        self.best_fidelity_history.append(self.best_fidelity)\n",
        "\n",
        "        if not improved:\n",
        "            # Check stagnation\n",
        "            if self.stagnant_fidelity_min <= self.best_fidelity < self.target_fidelity:\n",
        "                self.stagnant_counter += 1\n",
        "                logger.info(\n",
        "                    f\"Stagnant Counter Incremented: {self.stagnant_counter}/{self.stagnant_threshold} | Best Fidelity: {self.best_fidelity:.6f}\"\n",
        "                )\n",
        "            else:\n",
        "                self.stagnant_counter = 0\n",
        "\n",
        "        # Early stop if target_fidelity reached\n",
        "        if self.best_fidelity >= self.target_fidelity:\n",
        "            elapsed_time = time.perf_counter() - self.start_time\n",
        "            logger.info(\n",
        "                f\"Target fidelity of {self.best_fidelity:.6f} achieved in {self.episode_count} episodes. \"\n",
        "                f\"Training terminated after {elapsed_time:.2f} seconds.\"\n",
        "            )\n",
        "            # Save final best voltage\n",
        "            if self.best_voltage is not None:\n",
        "                best_voltage_final_path = os.path.join(self.results_dir, \"best_voltage_final.npy\")\n",
        "                np.save(best_voltage_final_path, self.best_voltage)\n",
        "                logger.info(f\"Final best voltage saved to '{best_voltage_final_path}'.\")\n",
        "\n",
        "            # Save Best Fidelity\n",
        "            best_fidelity_path = os.path.join(self.results_dir, \"best_fidelity.txt\")\n",
        "            with open(best_fidelity_path, \"w\") as f:\n",
        "                f.write(f\"Best Fidelity: {self.best_fidelity:.6f}\\n\")\n",
        "            logger.info(f\"Best fidelity saved to '{best_fidelity_path}'.\")\n",
        "\n",
        "            # Save Best Fidelity History\n",
        "            best_fidelity_history_path_npy = os.path.join(self.results_dir, \"best_fidelity_history.npy\")\n",
        "            best_fidelity_history_path_csv = os.path.join(self.results_dir, \"best_fidelity_history.csv\")\n",
        "            np.save(best_fidelity_history_path_npy, np.array(self.best_fidelity_history))\n",
        "            np.savetxt(best_fidelity_history_path_csv, np.array(self.best_fidelity_history), delimiter=\",\",\n",
        "                       header=\"Episode,Best_Fidelity\")\n",
        "            logger.info(f\"Best fidelity history saved to '{best_fidelity_history_path_npy}' and '{best_fidelity_history_path_csv}'.\")\n",
        "\n",
        "            self._plot_fidelity_progress(self.best_fidelity_history, self.results_dir)\n",
        "            return False  # Stop training\n",
        "\n",
        "        # Early stop if stagnating\n",
        "        if self.stagnant_counter >= self.stagnant_threshold:\n",
        "            elapsed_time = time.perf_counter() - self.start_time\n",
        "            logger.warning(\n",
        "                f\"Training is stagnating after {self.episode_count} episodes. \"\n",
        "                f\"Best Fidelity: {self.best_fidelity:.6f}. Terminating training after {elapsed_time:.2f} seconds.\"\n",
        "            )\n",
        "            if self.best_voltage is not None:\n",
        "                best_voltage_final_path = os.path.join(self.results_dir, \"best_voltage_final.npy\")\n",
        "                np.save(best_voltage_final_path, self.best_voltage)\n",
        "                logger.info(f\"Final best voltage saved to '{best_voltage_final_path}'.\")\n",
        "\n",
        "            # Save Best Fidelity\n",
        "            best_fidelity_path = os.path.join(self.results_dir, \"best_fidelity.txt\")\n",
        "            with open(best_fidelity_path, \"w\") as f:\n",
        "                f.write(f\"Best Fidelity: {self.best_fidelity:.6f}\\n\")\n",
        "            logger.info(f\"Best fidelity saved to '{best_fidelity_path}'.\")\n",
        "\n",
        "            # Save Best Fidelity History\n",
        "            best_fidelity_history_path_npy = os.path.join(self.results_dir, \"best_fidelity_history.npy\")\n",
        "            best_fidelity_history_path_csv = os.path.join(self.results_dir, \"best_fidelity_history.csv\")\n",
        "            np.save(best_fidelity_history_path_npy, np.array(self.best_fidelity_history))\n",
        "            np.savetxt(best_fidelity_history_path_csv, np.array(self.best_fidelity_history), delimiter=\",\",\n",
        "                       header=\"Episode,Best_Fidelity\")\n",
        "            logger.info(f\"Best fidelity history saved to '{best_fidelity_history_path_npy}' and '{best_fidelity_history_path_csv}'.\")\n",
        "\n",
        "            self._plot_fidelity_progress(self.best_fidelity_history, self.results_dir)\n",
        "            return False  # Stop training\n",
        "\n",
        "        elapsed_time = time.perf_counter() - self.start_time\n",
        "        logger.info(\n",
        "            f\"Training Status | Episodes: {self.episode_count} | Best Fidelity: {self.best_fidelity:.6f} | \"\n",
        "            f\"Stagnant Counter: {self.stagnant_counter}/{self.stagnant_threshold} | \"\n",
        "            f\"Elapsed Time: {elapsed_time:.2f} seconds\"\n",
        "        )\n",
        "\n",
        "        return True  # Continue training\n",
        "\n",
        "\n",
        "    def _plot_fidelity_progress(self, fidelity_history, results_dir):\n",
        "        \"\"\"\n",
        "        Plot and save the fidelity progress over episodes.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            episodes = np.arange(1, len(fidelity_history) + 1)\n",
        "            best_fidelity = np.array(fidelity_history)\n",
        "\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            plt.plot(episodes, best_fidelity, label='Best Fidelity')\n",
        "            plt.title('Global Best Fidelity Progress Over Episodes')\n",
        "            plt.xlabel('Episode')\n",
        "            plt.ylabel('Best Fidelity')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.tight_layout()\n",
        "            fidelity_plot_path = os.path.join(results_dir, \"fidelity_progress.png\")\n",
        "            plt.savefig(fidelity_plot_path)\n",
        "            plt.close()\n",
        "            logger.info(f\"Fidelity progress plot saved as '{fidelity_plot_path}'.\")\n",
        "\n",
        "            fidelity_progress_csv = os.path.join(results_dir, \"fidelity_progress.csv\")\n",
        "            df = pd.DataFrame({\n",
        "                'Episode': episodes,\n",
        "                'Best_Fidelity': best_fidelity\n",
        "            })\n",
        "            df.to_csv(fidelity_progress_csv, index=False)\n",
        "            logger.info(f\"Fidelity progress data saved as '{fidelity_progress_csv}'.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error while plotting or saving fidelity progress: {e}\")\n",
        "\n",
        "# ========================================\n",
        "# Part 8: NEW END-TO-END GRADIENT-BASED RL APPROACH (with batching + GPU)\n",
        "# ========================================\n",
        "class MLPPolicy:\n",
        "    \"\"\"\n",
        "    Minimal JAX MLP that outputs entire schedule (n_ch x piecewise_segments).\n",
        "    Incorporates mixed precision and is compatible with gradient checkpointing.\n",
        "    \"\"\"\n",
        "    def __init__(self, rng_key, n_ch=6, piecewise_segments=10, hidden_dim=64, latent_dim=8):\n",
        "        # **Changed**: piecewise_segments initialized to 10\n",
        "        self.n_ch = n_ch\n",
        "        self.piecewise_segments = piecewise_segments\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        rng_key, sub1, sub2 = jrandom.split(rng_key, 3)\n",
        "        W1 = 0.01 * jrandom.normal(sub1, (latent_dim, hidden_dim))\n",
        "        b1 = jnp.zeros((hidden_dim,), dtype=jnp.float32)\n",
        "        W2 = 0.01 * jrandom.normal(sub2, (hidden_dim, n_ch * piecewise_segments))\n",
        "        b2 = jnp.zeros((n_ch * piecewise_segments,), dtype=jnp.float32)\n",
        "        self.params = dict(W1=W1, b1=b1, W2=W2, b2=b2)\n",
        "\n",
        "    def forward(self, params, rng_key):\n",
        "        \"\"\"\n",
        "        Return controls of shape (n_ch, piecewise_segments).\n",
        "        \"\"\"\n",
        "        z = jrandom.normal(rng_key, (self.latent_dim,))\n",
        "        z = z.astype(jnp.float32)\n",
        "        W1 = params[\"W1\"].astype(jnp.float32)\n",
        "        b1 = params[\"b1\"].astype(jnp.float32)\n",
        "        W2 = params[\"W2\"].astype(jnp.float32)\n",
        "        b2 = params[\"b2\"].astype(jnp.float32)\n",
        "\n",
        "        h = jnp.tanh(z @ W1 + b1)\n",
        "        out = h @ W2 + b2\n",
        "        out = out.reshape((self.n_ch, self.piecewise_segments))\n",
        "        return out\n",
        "\n",
        "    def update_piecewise_segments(self, new_piecewise_segments, rng_key):\n",
        "        \"\"\"\n",
        "        Update the policy's piecewise_segments by initializing additional parameters.\n",
        "        \"\"\"\n",
        "        if new_piecewise_segments <= self.piecewise_segments:\n",
        "            logger.warning(\"New piecewise_segments must be greater than the current value.\")\n",
        "            return\n",
        "\n",
        "        additional_segments = new_piecewise_segments - self.piecewise_segments\n",
        "        rng_key, subkey1, subkey2 = jrandom.split(rng_key, 3)\n",
        "\n",
        "        # Initialize new W2 and b2 for additional segments\n",
        "        new_W2 = 0.01 * jrandom.normal(subkey1, (self.hidden_dim, self.n_ch * additional_segments))\n",
        "        new_b2 = jnp.zeros((self.n_ch * additional_segments,), dtype=jnp.float32)\n",
        "\n",
        "        # Concatenate existing and new parameters\n",
        "        updated_W2 = jnp.concatenate([self.params[\"W2\"], new_W2], axis=1)\n",
        "        updated_b2 = jnp.concatenate([self.params[\"b2\"], new_b2], axis=0)\n",
        "\n",
        "        self.params[\"W2\"] = updated_W2\n",
        "        self.params[\"b2\"] = updated_b2\n",
        "        self.piecewise_segments = new_piecewise_segments\n",
        "        logger.info(f\"Updated piecewise_segments to {new_piecewise_segments}.\")\n",
        "\n",
        "def single_seed_fidelity(params, rng_key, config, policy):\n",
        "    \"\"\"\n",
        "    Evaluate final fidelity for a single random seed, given the current policy params.\n",
        "    \"\"\"\n",
        "    # Generate controls from policy\n",
        "    voltages = policy.forward(params, rng_key)\n",
        "    V0_t_list = voltages\n",
        "    V1_t_list = voltages\n",
        "\n",
        "    fidelity_all = compute_multi_qubit_fidelity_closed_system(\n",
        "        V0_t_list, V1_t_list,\n",
        "        config[\"APIC_params\"][\"L\"],\n",
        "        config[\"APIC_params\"][\"n0\"],\n",
        "        config[\"APIC_params\"][\"lambda_0\"],\n",
        "        config[\"APIC_params\"][\"a0\"],\n",
        "        config[\"APIC_params\"][\"t0\"],\n",
        "        config[\"APIC_params\"][\"a1\"],\n",
        "        config[\"APIC_params\"][\"t1\"],\n",
        "        config[\"APIC_params\"][\"phase_mod\"],\n",
        "        config[\"APIC_params\"][\"amp_mod\"],\n",
        "\n",
        "        config[\"system_params\"][\"delta\"],\n",
        "        config[\"atom_beam_params\"][\"atom_positions\"],\n",
        "        config[\"atom_beam_params\"][\"dipoles\"],\n",
        "        config[\"atom_beam_params\"][\"beam_centers\"],\n",
        "        config[\"atom_beam_params\"][\"beam_waist\"],\n",
        "        config[\"atom_beam_params\"][\"X\"],\n",
        "        config[\"atom_beam_params\"][\"Y\"],\n",
        "        config[\"atom_beam_params\"][\"Omega_prefactor_MHz\"],\n",
        "\n",
        "        config[\"control_Vt_params\"][\"t_steps\"],\n",
        "        config[\"control_Vt_params\"][\"dt\"],\n",
        "\n",
        "        config[\"system_params\"][\"N_ch\"],\n",
        "        config[\"system_params\"][\"distances\"],\n",
        "        config[\"system_params\"][\"coupling_lengths\"],\n",
        "        config[\"system_params\"][\"n_eff_list\"],\n",
        "        config[\"system_params\"][\"kappa0\"],\n",
        "        config[\"system_params\"][\"alpha\"],\n",
        "        config[\"system_params\"][\"enable_crosstalk\"],\n",
        "\n",
        "        config[\"system_params\"][\"N_slm\"],\n",
        "        config[\"system_params\"][\"N_ch_slm_in\"],\n",
        "        config[\"system_params\"][\"N_scat_1\"],\n",
        "        config[\"system_params\"][\"N_scat_2\"],\n",
        "        config[\"system_params\"][\"N_a\"],\n",
        "        config[\"system_params\"][\"N_qubit_level\"],\n",
        "        config[\"system_params\"][\"omega_0\"],\n",
        "        config[\"system_params\"][\"omega_r\"],\n",
        "        config[\"system_params\"][\"a_pic\"],\n",
        "        config[\"system_params\"][\"a_scat_1\"],\n",
        "        config[\"system_params\"][\"U_target\"],\n",
        "\n",
        "        config[\"system_params\"].get(\"gate_type\", \"single\"),\n",
        "    )\n",
        "\n",
        "    return fidelity_all[-1]  # scalar\n",
        "\n",
        "def build_loss_fn(policy, config):\n",
        "    \"\"\"\n",
        "    Returns a function loss_fn(params, rng_key) -> scalar,\n",
        "    that runs multiple seeds in parallel with vmap and then returns avg cost = (1 - avg_fidelity).\n",
        "    \"\"\"\n",
        "    batch_size = config.get(\"batch_size\", 2)\n",
        "\n",
        "    # Define a single-seed function that depends on policy via closure\n",
        "    def single_seed_loss(param, subkey):\n",
        "        fid = single_seed_fidelity(param, subkey, config, policy)\n",
        "        loss = jnp.where(fid <= 1.0, 1.0 - fid, (fid - 1.0) ** 2)\n",
        "        return loss\n",
        "\n",
        "    # Vectorize over the 'rng_keys' dimension to handle multiple seeds\n",
        "    batched_seed_loss = jax.vmap(single_seed_loss, in_axes=(None, 0))\n",
        "\n",
        "    def loss_fn(params, step_rng_key):\n",
        "        rng_keys = jrandom.split(step_rng_key, batch_size)  # shape=(batch_size,)\n",
        "        losses = batched_seed_loss(params, rng_keys)        # shape=(batch_size,)\n",
        "        return jnp.mean(losses)  # average cost across seeds\n",
        "\n",
        "    return loss_fn\n",
        "\n",
        "def train_end_to_end_grad(\n",
        "    policy, config, num_iterations=500, initial_lr=1e-3, patience=50, min_lr=1e-6, lr_decay_factor=0.5,\n",
        "    use_mixed_precision=True, use_checkpointing=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Optimized end-to-end gradient-based training using JAX.\n",
        "\n",
        "    Parameters:\n",
        "        policy (MLPPolicy): The policy instance to train.\n",
        "        config (dict): Configuration parameters.\n",
        "        num_iterations (int): Number of training iterations.\n",
        "        initial_lr (float): Initial learning rate.\n",
        "        patience (int): Number of iterations to wait before reducing learning rate.\n",
        "        min_lr (float): Minimum learning rate.\n",
        "        lr_decay_factor (float): Factor to decay learning rate.\n",
        "        use_mixed_precision (bool): Whether to use mixed precision.\n",
        "        use_checkpointing (bool): Whether to use gradient checkpointing.\n",
        "\n",
        "    Returns:\n",
        "        dict: Trained policy parameters.\n",
        "    \"\"\"\n",
        "    key = jrandom.PRNGKey(42)\n",
        "    loss_fn = build_loss_fn(policy, config)\n",
        "\n",
        "    # Initialize optimizer with initial learning rate and gradient clipping\n",
        "    optimizer = optax.chain(\n",
        "        optax.clip_by_global_norm(1.0),  # Gradient clipping\n",
        "        optax.adam(initial_lr)\n",
        "    )\n",
        "    opt_state = optimizer.init(policy.params)\n",
        "\n",
        "    best_fidelity = 0.0\n",
        "    best_voltage = None\n",
        "    fidelity_history = []\n",
        "    stagnant_counter = 0\n",
        "    current_lr = initial_lr\n",
        "\n",
        "    logger.info(f\"[End-to-End Grad] Starting training for {num_iterations} iterations, initial_lr={initial_lr}, batch_size={config.get('batch_size', 4)}\")\n",
        "\n",
        "    # Define the loss function with optional checkpointing\n",
        "    if use_checkpointing:\n",
        "        def loss_with_checkpoint(params, step_rng_key):\n",
        "            return jax.checkpoint(loss_fn, prevent_cse=True)(params, step_rng_key)\n",
        "        loss_fn_jit = jax.jit(loss_with_checkpoint)\n",
        "    else:\n",
        "        loss_fn_jit = jax.jit(loss_fn)\n",
        "\n",
        "    for i in range(1, num_iterations + 1):\n",
        "        key, subkey = jrandom.split(key)\n",
        "        grads = jax.grad(loss_fn_jit)(policy.params, subkey)\n",
        "        updates, opt_state = optimizer.update(grads, opt_state, policy.params)\n",
        "        policy.params = optax.apply_updates(policy.params, updates)\n",
        "        current_loss = loss_fn_jit(policy.params, subkey)\n",
        "\n",
        "        # Compute fidelity from current_loss\n",
        "        fidelity = 1.0 - float(current_loss)\n",
        "\n",
        "        if fidelity < 0.0 or fidelity > 1.0:\n",
        "            continue  # Skip updating best_fidelity\n",
        "\n",
        "        # Update best fidelity and save V(t) if improved\n",
        "        if fidelity > best_fidelity:\n",
        "            best_fidelity = fidelity\n",
        "            best_voltage = policy.forward(policy.params, subkey)\n",
        "            # Reset stagnant counter since improvement was observed\n",
        "            stagnant_counter = 0\n",
        "        else:\n",
        "            stagnant_counter += 1\n",
        "            logger.info(f\"[End-to-End Grad] Iter={i}, no improvement in fidelity. Stagnant Counter: {stagnant_counter}/{patience}\")\n",
        "\n",
        "        # Record fidelity history\n",
        "        fidelity_history.append(best_fidelity)\n",
        "\n",
        "        # Check if patience is exceeded to decay learning rate\n",
        "        if stagnant_counter >= patience:\n",
        "            if current_lr > min_lr:\n",
        "                current_lr = max(current_lr * lr_decay_factor, min_lr)\n",
        "                optimizer = optax.chain(\n",
        "                    optax.clip_by_global_norm(1.0),\n",
        "                    optax.adam(current_lr)\n",
        "                )\n",
        "                opt_state = optimizer.init(policy.params)\n",
        "                logger.info(f\"[End-to-End Grad] Iter={i}, learning rate decayed to {current_lr}. Resetting stagnant counter.\")\n",
        "                stagnant_counter = 0  # Reset counter after decay\n",
        "            else:\n",
        "                logger.warning(f\"[End-to-End Grad] Iter={i}, minimum learning rate reached ({min_lr}).\")\n",
        "\n",
        "        # Log every 10 iterations\n",
        "        if (i % 10) == 0:\n",
        "            logger.info(f\"[End-to-End Grad] Iter={i}, loss={current_loss:.6f}, best_fidelity={best_fidelity:.6f}, current_lr={current_lr:.6f}\")\n",
        "\n",
        "            if best_fidelity > config.get(\"target_fidelity\", 0.999):\n",
        "                logger.info(f\"[End-to-End Grad] Best fidelity > {config.get('target_fidelity', 0.999)} reached at iter={i}. Stopping early!\")\n",
        "                break\n",
        "\n",
        "    # After training, save fidelity history\n",
        "    fidelity_history_path_npy = os.path.join(config[\"results_dir\"], \"fidelity_history.npy\")\n",
        "    fidelity_history_path_csv = os.path.join(config[\"results_dir\"], \"fidelity_history.csv\")\n",
        "    np.save(fidelity_history_path_npy, np.array(fidelity_history))\n",
        "    np.savetxt(fidelity_history_path_csv, np.array(fidelity_history), delimiter=\",\", header=\"Episode,Best_Fidelity\")\n",
        "    logger.info(f\"Fidelity history saved to '{fidelity_history_path_npy}' and '{fidelity_history_path_csv}'.\")\n",
        "\n",
        "    # Plot fidelity progress\n",
        "    _plot_fidelity_progress(fidelity_history, config[\"results_dir\"])\n",
        "\n",
        "    # Save the trained model parameters\n",
        "    model_save_path = os.path.join(config.get('results_dir', 'results'), \"trained_model_params.pkl\")\n",
        "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)  # Ensure directory exists\n",
        "    with open(model_save_path, \"wb\") as f:\n",
        "        pickle.dump(policy.params, f)\n",
        "    logger.info(f\"Trained model parameters saved to '{model_save_path}'.\")\n",
        "\n",
        "    return policy.params\n",
        "\n",
        "\n",
        "def _plot_fidelity_progress(fidelity_history, results_dir):\n",
        "    \"\"\"\n",
        "    Helper function to plot and save fidelity progress.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        episodes = np.arange(1, len(fidelity_history) + 1)\n",
        "        best_fidelity = np.array(fidelity_history)\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(episodes, best_fidelity, label='Best Fidelity')\n",
        "        plt.title('Global Best Fidelity Progress Over Episodes')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Best Fidelity')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        fidelity_plot_path = os.path.join(results_dir, \"fidelity_progress.png\")\n",
        "        plt.savefig(fidelity_plot_path)\n",
        "        plt.close()\n",
        "        logger.info(f\"Fidelity progress plot saved as '{fidelity_plot_path}'.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error while plotting or saving fidelity progress: {e}\")\n",
        "\n",
        "# ========================================\n",
        "# Part 9: Loading and Utilizing the Saved Model Parameters\n",
        "# ========================================\n",
        "def load_trained_model(model_path):\n",
        "    \"\"\"\n",
        "    Load trained model parameters from a pickle file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(model_path, \"rb\") as f:\n",
        "            params = pickle.load(f)\n",
        "        return params\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to load trained model parameters from '{model_path}': {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# ========================================\n",
        "# Part 10: Modified Main Function\n",
        "# ========================================\n",
        "def main():\n",
        "    start_time = time.perf_counter()  # Define start_time for total execution time logging\n",
        "\n",
        "    results_dir = \"results\"\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "    # Configure logging\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    log_file = os.path.join(results_dir, \"training_log.txt\")\n",
        "    file_handler = logging.FileHandler(log_file)\n",
        "    file_handler.setLevel(logging.INFO)\n",
        "    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "    file_handler.setFormatter(file_formatter)\n",
        "    if not logger.handlers:\n",
        "        logger.addHandler(file_handler)\n",
        "\n",
        "    # Force JAX to use GPU if available\n",
        "    jax.config.update(\"jax_platform_name\", \"gpu\")   # Use GPU\n",
        "    # Enable dynamic memory allocation on GPU\n",
        "    os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"  # Prevent pre-allocation of GPU memory\n",
        "    # Optionally limit JAX to use fewer CPU threads if running in HPC environment\n",
        "    # os.environ[\"JAX_NUM_THREADS\"] = \"1\"\n",
        "\n",
        "    # Create configuration\n",
        "    config = create_config()  # Now initializes with piecewise_segments=10\n",
        "    config[\"batch_size\"] = 4  # Adjusted for optimal batching\n",
        "    config[\"results_dir\"] = results_dir  # Ensure results_dir is in config\n",
        "\n",
        "    # Define adaptive training phases with increasing t_steps and piecewise_segments\n",
        "    training_phases = [\n",
        "        {\"t_steps\": 20, \"piecewise_segments\": 10, \"num_iterations\": 500},\n",
        "        {\"t_steps\": 50, \"piecewise_segments\": 25, \"num_iterations\": 500},\n",
        "        {\"t_steps\": 100, \"piecewise_segments\": 50, \"num_iterations\": 1000},\n",
        "        # Add more phases as needed\n",
        "    ]\n",
        "\n",
        "    # Initialize policy parameters to None for the first phase\n",
        "    initial_params = None\n",
        "    policy = None  # To hold the policy instance across phases\n",
        "    rng_key = jrandom.PRNGKey(0)  # Initialize RNG key\n",
        "\n",
        "    for phase_idx, phase in enumerate(training_phases, 1):\n",
        "        t_steps = int(phase[\"t_steps\"])\n",
        "        piecewise_segments = int(phase[\"piecewise_segments\"])\n",
        "        num_iterations = phase[\"num_iterations\"]\n",
        "\n",
        "        # Update config with current phase's t_steps and dt\n",
        "        config[\"control_Vt_params\"][\"t_steps\"] = t_steps\n",
        "        config[\"control_Vt_params\"][\"dt\"] = config[\"control_Vt_params\"][\"tmax\"] / t_steps\n",
        "\n",
        "        logger.info(f\"Starting Training Phase {phase_idx}: t_steps={t_steps}, piecewise_segments={piecewise_segments}, num_iterations={num_iterations}\")\n",
        "\n",
        "        # Initialize or update the policy\n",
        "        if phase_idx == 1:\n",
        "            # Initialize the policy for the first phase\n",
        "            policy = MLPPolicy(rng_key, piecewise_segments=piecewise_segments)\n",
        "        else:\n",
        "            # Update piecewise_segments for subsequent phases\n",
        "            rng_key, subkey = jrandom.split(rng_key)\n",
        "            policy.update_piecewise_segments(piecewise_segments, subkey)\n",
        "\n",
        "        # Train the model for the current phase\n",
        "        trained_params = train_end_to_end_grad(\n",
        "            policy=policy,  # Pass the existing policy\n",
        "            config=config,\n",
        "            num_iterations=num_iterations,\n",
        "            initial_lr=1e-3,\n",
        "            patience=500,  # Updated patience\n",
        "            min_lr=1e-6,\n",
        "            lr_decay_factor=0.5,\n",
        "            use_mixed_precision=True,\n",
        "            use_checkpointing=True\n",
        "        )\n",
        "\n",
        "        # Update the policy's parameters\n",
        "        policy.params = trained_params\n",
        "\n",
        "        # Set the trained parameters as the initial parameters for the next phase\n",
        "        initial_params = trained_params\n",
        "\n",
        "    logger.info(\"Adaptive Training with Increasing t_steps and piecewise_segments Completed.\")\n",
        "\n",
        "    # Loading and utilizing the saved model parameters\n",
        "    trained_model_path = os.path.join(config[\"results_dir\"], \"trained_model_params.pkl\")\n",
        "    if os.path.exists(trained_model_path):\n",
        "        loaded_params = load_trained_model(trained_model_path)\n",
        "        if loaded_params is not None:\n",
        "            logger.info(f\"Loaded trained model parameters from '{trained_model_path}'.\")\n",
        "\n",
        "            # Assign loaded parameters to the existing policy\n",
        "            policy.params = loaded_params\n",
        "\n",
        "            # Generate and save optimal V(t) using the loaded parameters\n",
        "            new_rng_key = jrandom.PRNGKey(123)  # Example seed\n",
        "            optimal_Vt = policy.forward(loaded_params, new_rng_key)\n",
        "\n",
        "            # Save the generated optimal V(t)\n",
        "            optimal_Vt_path = os.path.join(config[\"results_dir\"], \"optimal_Vt_new_seed.npy\")\n",
        "            np.save(optimal_Vt_path, optimal_Vt)\n",
        "            logger.info(f\"Optimal V(t) for new seed saved to '{optimal_Vt_path}'.\")\n",
        "\n",
        "        else:\n",
        "            logger.error(f\"Failed to load trained model parameters from '{trained_model_path}'.\")\n",
        "    else:\n",
        "        logger.error(f\"Trained model parameters file '{trained_model_path}' not found.\")\n",
        "\n",
        "    # Log total execution time for adaptive training approach\n",
        "    end_time = time.perf_counter()\n",
        "    total_time = end_time - start_time\n",
        "    logger.info(f\"Adaptive training with increasing t_steps and piecewise_segments completed in {total_time/60:.2f} minutes.\")\n",
        "\n",
        "# ========================================\n",
        "# Entry Point\n",
        "# ========================================\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
